purpose
-------

The gltr package uses a **Krylov-subspace iteration** to find an
approximation of the global minimizer of **quadratic objective function** 
within an **ellipsoidal region**; this is commonly known as the
**trust-region subproblem**.
The aim is to minimize the quadratic objective function
$$q(x) = f + g^T x + \frac{1}{2} x^T H x,$$ 
where the vector $x$ is required to satisfy 
the ellipsoidal  **trust-region constraint** $\|x\|_{M} \leq  \Delta$, 
where the $M$-norm of $x$ is defined to be $\|x\|_{M} = \sqrt{x^T M x}$,
and where the **radius** $\Delta > 0$.
The method may be suitable for large problems as no factorization of $H$ is
required. Reverse communication is used to obtain
matrix-vector products of the form $H z$ and $M^{-1} z.$

See Section 4 of $GALAHAD/doc/gltr.pdf for additional details.

method
------

The required solution $x$ necessarily satisfies the optimality condition
$H x + \lambda M x + g = 0$, where $\lambda \geq 0$ is a Lagrange multiplier 
corresponding to the constraint $\|x\|_M  \leq  \Delta$.
In addition, the matrix $H + \lambda M$ will be positive definite.

The method is iterative. Starting  with the vector $M^{-1} g$,
a matrix of Lanczos vectors is built one column at a time
so that the $k$-th column is generated during
iteration $k$. These columns span a so-called Krylov space.
The resulting $n$ by $k$ matrix $Q_k $ has the
property that $Q_{k}^T H Q_k^{}  =  T_{k}^{}$,
where $T_k$ is tridiagonal. An approximation to the
required solution may then be expressed formally as
$$x_{k+1}  =  Q_k y_k$$
where $y_k $ solves the ``tridiagonal'' subproblem of minimizing
$$\frac{1}{2}  y^T T_k y  + \|g\|_{M^{-1} } e_{1}^T y \;\;\;
\mbox{subject to the constraint}\;\; \|y\|_2  \leq  \Delta,\;\;\mbox{(1)}$$
and where $e_1$ is the first unit vector.

If the solution to (1) lies interior to the constraint, the required
solution $x_{k+1}$ may simply be found as the $k$-th (preconditioned)
conjugate-gradient iterate. This solution can be obtained without the need to
access the whole matrix $Q_k$.
These conjugate-gradient iterates increase in $M$-norm, and
thus once one of them exceeds $\Delta$ in $M$-norm, the solution must occur
on the constraint boundary. Thereafter, the solution to (1) is less
easy to obtain, but an efficient inner iteration to solve (1) is
nonetheless achievable because $T_k $ is tridiagonal.
It is possible to observe the optimality measure
$\|H x  +  \lambda M x  +  g\|_{M^{-1}}$
without computing $x_{k+1}$, and thus without
needing $Q_k $. Once this measure is sufficiently small, a second pass
is required to obtain the estimate $x_{k+1} $ from $y_k $.
As this second pass is an additional expense, a record is kept of the
optimal objective function values for each value of $k$, and the second
pass is only performed so far as to ensure a given fraction of the
final optimal objective value. Large savings may be made in the second
pass by choosing the required fraction to be significantly smaller than one.

A cheaper alternative is to use the Steihuag-Toint strategy, which is simply
to stop at the first boundary point encountered along the piecewise
linear path generated by the conjugate-gradient iterates. Note that if
$H$ is significantly indefinite, this strategy often produces a far from
optimal point, but is effective when $H$ is positive definite or almost
so.

reference
---------

The method is described in detail in

  N. I. M. Gould, S. Lucidi, M. Roma and Ph. L. Toint,
  ``Solving the trust-region subproblem using the Lanczos method''.
  *SIAM Journal on Optimization* **9(2)** (1999), 504-525.
